---
layout:     post
title:    PyTorch学习笔记-03
subtitle:   PyTorch入门-2
date:       2018-05-08
author:     hmoytx
header-img: img/bg_02.jpg
catalog: true
tags:
    - python
    - PyTorch 
    - machine learning
---

# 快速入门
## Autograd
深度学习的算法本质上是通过反向传播求导数，PyTorch的Autograd模块实现了此功能。模块自动提供微分，避免了手动计算导数的过程。autograd.Variable是Autograd中的核心类，它简单封装了Tensor，支持几乎所有的Tensor操作，在封装为Variable后可以调用它的.backward实现反向传播，通过.grad查看梯度。

现在还是在ipython中演示一下基本的操作
```python
In [1]: import torch as t

In [2]: from torch.autograd import Variable

In [3]: x = Variable(t.ones(3,3), requires_grad=True) #需要设置参数requires_grad为True，否则无法进行反向传播

In [4]: x
Out[4]:
tensor([[ 1.,  1.,  1.],
        [ 1.,  1.,  1.],
        [ 1.,  1.,  1.]])

In [5]: y = x.sum()

In [6]: y
Out[6]: tensor(9.)In [7]: y.backward()

In [8]: x.grad
Out[8]:
tensor([[ 1.,  1.,  1.],
        [ 1.,  1.,  1.],
        [ 1.,  1.,  1.]])

In [9]: y.backward()   #实际使用中要注意，grad在反向传播过程中是累加的，所以在每次运行反向传播，梯度都会累加之前的梯度，所以反向传播之前需要把梯度清零  

In [10]: x.grad
Out[10]:
tensor([[ 2.,  2.,  2.],
        [ 2.,  2.,  2.],
        [ 2.,  2.,  2.]])

In [11]: x.grad.data.zero_()  #清零操作
Out[11]:
tensor([[ 0.,  0.,  0.],
        [ 0.,  0.,  0.],
        [ 0.,  0.,  0.]])
```

# 注意
限于本咸鱼的水平只能暂时讲这一些简单的东西其实Variable还有很多的细节，有想要了解的可以去查阅相关手册或者其他资料
